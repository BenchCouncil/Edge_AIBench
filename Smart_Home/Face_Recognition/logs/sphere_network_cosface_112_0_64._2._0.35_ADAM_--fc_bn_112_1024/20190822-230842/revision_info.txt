arguments: train/train_multi_gpu.py --logs_base_dir logs/sphere_network_cosface_112_0_64._2._0.35_ADAM_--fc_bn_112_1024/ --models_base_dir models/sphere_network_cosface_112_0_64._2._0.35_ADAM_--fc_bn_112_1024/ --data_dir dataset/casia-112x112 --list_file dataset/cleaned_list.txt --model_def models.inception_resnet_v1 --optimizer ADAM --learning_rate -1 --max_nrof_epochs 100 --random_flip --learning_rate_schedule_file lr_coco.txt --num_gpus 1 --weight_decay 1e-4 --loss_type cosface --scale 64. --weight 2. --alpha 0.35 --network sphere_network --fc_bn --image_height 112 --image_width 112 --embedding_size 1024
--------------------
git hash: b'4a1857ec586fe9ddb6175165330b1ae7a9b9a79a'
--------------------
b'diff --git a/lib/utils.py b/lib/utils.py\nindex 0e4d7ac..02abdd9 100644\n--- a/lib/utils.py\n+++ b/lib/utils.py\n@@ -583,6 +583,8 @@ def dataset_from_list(data_dir,list_file):\n     count = 1\n     class_paths = {}\n     for line in lines:\n+        line=line.replace("\\\\",\'/\')\n+        print(line)\n         image_path, _ = line.split(\' \')\n         class_name, _ = image_path.split(\'/\')\n         if class_name not in class_paths:\n@@ -591,8 +593,9 @@ def dataset_from_list(data_dir,list_file):\n         assert os.path.exists(full_image_path), \'file {} not exist\'.format(full_image_path)\n         class_paths[class_name].append(full_image_path)\n     dataset = []\n-    keys = class_paths.keys()\n-    keys.sort()\n+    #keys = class_paths.keys()\n+    #keys.sort()\n+    keys = sorted(class_paths)\n     for key in keys:\n         dataset.append(ImageClass(key,class_paths[key]))\n     return dataset\n@@ -802,5 +805,5 @@ def put_images_on_grid(images, shape=(16,8)):\n \n def write_arguments_to_file(args, filename):\n     with open(filename, \'w\') as f:\n-        for key, value in vars(args).iteritems():\n+        for key, value in vars(args).items():\n             f.write(\'%s: %s\\n\' % (key, str(value)))\ndiff --git a/test.sh b/test.sh\nindex d0e9e22..0536eb8 100755\n--- a/test.sh\n+++ b/test.sh\n@@ -1,7 +1,8 @@\n V="v1"\n if test ${V} = "v1"\n then\n-  MODEL_DIR=models/model-20180309-083949.ckpt-60000\n+  MODEL_DIR=models/sphere_network_cosface_112_0_64._2._0.35_ADAM_--fc_bn_112_1024/20190822-123648/model-20190822-123648.ckpt-60000\n+  #MODEL_DIR=models/model-20180309-083949.ckpt-60000\n   TEST_DATA=dataset/lfw-112X96\n   EMBEDDING_SIZE=512\n   FC_BN=\'\'\ndiff --git a/train/train_multi_gpu.py b/train/train_multi_gpu.py\nindex cab3879..f4ffcf5 100644\n--- a/train/train_multi_gpu.py\n+++ b/train/train_multi_gpu.py\n@@ -13,7 +13,7 @@ sys.path.insert(0,\'networks\')\n import tensorflow as tf\n from tensorflow.python.client import timeline\n from tensorflow.contrib import slim\n-import tensorflow.contrib.data as tf_data\n+import tensorflow.data as tf_data\n from collections import Counter\n import numpy as np\n import importlib\n@@ -73,7 +73,7 @@ def main(args):\n \n     dataset_size = len(image_list)\n     single_batch_size = args.people_per_batch*args.images_per_person\n-    indices = range(dataset_size)\n+    indices = list(range(dataset_size))\n     np.random.shuffle(indices)\n \n     def _sample_people_softmax(x):\n@@ -156,7 +156,7 @@ def main(args):\n             softmax_dataset = tf_data.Dataset.range(args.epoch_size*args.max_nrof_epochs*100)\n             softmax_dataset = softmax_dataset.map(lambda x: tf.py_func(_sample_people_softmax,[x],[tf.string,tf.int32]))\n             softmax_dataset = softmax_dataset.flat_map(_from_tensor_slices)\n-            softmax_dataset = softmax_dataset.map(_parse_function,num_threads=8,output_buffer_size=2000)\n+            softmax_dataset = softmax_dataset.map(_parse_function)\n             softmax_dataset = softmax_dataset.batch(args.num_gpus*single_batch_size)\n             softmax_iterator = softmax_dataset.make_initializable_iterator()\n             softmax_next_element = softmax_iterator.get_next()'