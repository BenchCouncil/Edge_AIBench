Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='small', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 9816.457509994507
load data time: 0.28896331787109375
process data time: 176.82695388793945

32/32 [==============================] - 1s 22ms/step
The inference time is  713.7868404388428
save time is: 0.6189346313476562
total time is  10711.844682693481
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 7806.80775642395
load data time: 3.592967987060547
process data time: 29756.561994552612

  32/3236 [..............................] - ETA: 49s
  96/3236 [..............................] - ETA: 18s
 160/3236 [>.............................] - ETA: 11s
 224/3236 [=>............................] - ETA: 8s 
 288/3236 [=>............................] - ETA: 7s
 352/3236 [==>...........................] - ETA: 6s
 416/3236 [==>...........................] - ETA: 5s
 480/3236 [===>..........................] - ETA: 5s
 544/3236 [====>.........................] - ETA: 4s
 608/3236 [====>.........................] - ETA: 4s
 672/3236 [=====>........................] - ETA: 4s
 736/3236 [=====>........................] - ETA: 3s
 800/3236 [======>.......................] - ETA: 3s
 864/3236 [=======>......................] - ETA: 3s
 928/3236 [=======>......................] - ETA: 3s
 992/3236 [========>.....................] - ETA: 3s
1056/3236 [========>.....................] - ETA: 3s
1120/3236 [=========>....................] - ETA: 2s
1184/3236 [=========>....................] - ETA: 2s
1248/3236 [==========>...................] - ETA: 2s
1312/3236 [===========>..................] - ETA: 2s
1376/3236 [===========>..................] - ETA: 2s
1440/3236 [============>.................] - ETA: 2s
1504/3236 [============>.................] - ETA: 2s
1568/3236 [=============>................] - ETA: 2s
1632/3236 [==============>...............] - ETA: 2s
1696/3236 [==============>...............] - ETA: 1s
1760/3236 [===============>..............] - ETA: 1s
1824/3236 [===============>..............] - ETA: 1s
1888/3236 [================>.............] - ETA: 1s
1952/3236 [=================>............] - ETA: 1s
2016/3236 [=================>............] - ETA: 1s
2080/3236 [==================>...........] - ETA: 1s
2144/3236 [==================>...........] - ETA: 1s
2208/3236 [===================>..........] - ETA: 1s
2272/3236 [====================>.........] - ETA: 1s
2336/3236 [====================>.........] - ETA: 1s
2400/3236 [=====================>........] - ETA: 1s
2464/3236 [=====================>........] - ETA: 0s
2496/3236 [======================>.......] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2624/3236 [=======================>......] - ETA: 0s
2688/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2816/3236 [=========================>....] - ETA: 0s
2880/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3072/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3200/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 4s 1ms/step
The inference time is  4057.3740005493164
save time is: 10.03408432006836
total time is  41636.74783706665
