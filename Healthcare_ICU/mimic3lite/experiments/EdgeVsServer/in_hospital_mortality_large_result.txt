Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 3468.7159061431885
load data time: 4.895210266113281
process data time: 34178.90238761902

  32/3236 [..............................] - ETA: 42s
  96/3236 [..............................] - ETA: 16s
 160/3236 [>.............................] - ETA: 11s
 224/3236 [=>............................] - ETA: 9s 
 288/3236 [=>............................] - ETA: 7s
 352/3236 [==>...........................] - ETA: 6s
 416/3236 [==>...........................] - ETA: 6s
 480/3236 [===>..........................] - ETA: 5s
 544/3236 [====>.........................] - ETA: 5s
 608/3236 [====>.........................] - ETA: 4s
 640/3236 [====>.........................] - ETA: 4s
 672/3236 [=====>........................] - ETA: 4s
 736/3236 [=====>........................] - ETA: 4s
 800/3236 [======>.......................] - ETA: 4s
 864/3236 [=======>......................] - ETA: 4s
 928/3236 [=======>......................] - ETA: 3s
 992/3236 [========>.....................] - ETA: 3s
1056/3236 [========>.....................] - ETA: 3s
1120/3236 [=========>....................] - ETA: 3s
1184/3236 [=========>....................] - ETA: 3s
1248/3236 [==========>...................] - ETA: 3s
1312/3236 [===========>..................] - ETA: 2s
1376/3236 [===========>..................] - ETA: 2s
1440/3236 [============>.................] - ETA: 2s
1504/3236 [============>.................] - ETA: 2s
1568/3236 [=============>................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1664/3236 [==============>...............] - ETA: 2s
1728/3236 [===============>..............] - ETA: 2s
1792/3236 [===============>..............] - ETA: 2s
1856/3236 [================>.............] - ETA: 1s
1920/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 1s
2368/3236 [====================>.........] - ETA: 1s
2432/3236 [=====================>........] - ETA: 1s
2496/3236 [======================>.......] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2624/3236 [=======================>......] - ETA: 0s
2688/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2816/3236 [=========================>....] - ETA: 0s
2880/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3040/3236 [===========================>..] - ETA: 0s
3104/3236 [===========================>..] - ETA: 0s
3168/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 4s 1ms/step
The inference time is  4424.77560043335
save time is: 12.218236923217773
total time is  42093.66869926453
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 6640.286445617676
load data time: 4.87065315246582
process data time: 26100.68130493164

  32/3236 [..............................] - ETA: 56s
  96/3236 [..............................] - ETA: 20s
 160/3236 [>.............................] - ETA: 13s
 224/3236 [=>............................] - ETA: 10s
 288/3236 [=>............................] - ETA: 8s 
 352/3236 [==>...........................] - ETA: 7s
 416/3236 [==>...........................] - ETA: 6s
 512/3236 [===>..........................] - ETA: 5s
 576/3236 [====>.........................] - ETA: 4s
 672/3236 [=====>........................] - ETA: 4s
 768/3236 [======>.......................] - ETA: 3s
 864/3236 [=======>......................] - ETA: 3s
 928/3236 [=======>......................] - ETA: 3s
1024/3236 [========>.....................] - ETA: 3s
1088/3236 [=========>....................] - ETA: 2s
1184/3236 [=========>....................] - ETA: 2s
1280/3236 [==========>...................] - ETA: 2s
1344/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 2s
1472/3236 [============>.................] - ETA: 2s
1568/3236 [=============>................] - ETA: 1s
1632/3236 [==============>...............] - ETA: 1s
1728/3236 [===============>..............] - ETA: 1s
1792/3236 [===============>..............] - ETA: 1s
1888/3236 [================>.............] - ETA: 1s
1952/3236 [=================>............] - ETA: 1s
2016/3236 [=================>............] - ETA: 1s
2080/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 0s
2368/3236 [====================>.........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2848/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3040/3236 [===========================>..] - ETA: 0s
3104/3236 [===========================>..] - ETA: 0s
3168/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 975us/step
The inference time is  3159.013509750366
save time is: 10.147571563720703
total time is  35919.10719871521
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 5612.435817718506
load data time: 3.777742385864258
process data time: 29834.583044052124

  32/3236 [..............................] - ETA: 1:10
  96/3236 [..............................] - ETA: 26s 
 160/3236 [>.............................] - ETA: 16s
 224/3236 [=>............................] - ETA: 12s
 256/3236 [=>............................] - ETA: 11s
 320/3236 [=>............................] - ETA: 9s 
 384/3236 [==>...........................] - ETA: 8s
 416/3236 [==>...........................] - ETA: 8s
 480/3236 [===>..........................] - ETA: 7s
 544/3236 [====>.........................] - ETA: 6s
 608/3236 [====>.........................] - ETA: 6s
 672/3236 [=====>........................] - ETA: 5s
 736/3236 [=====>........................] - ETA: 5s
 800/3236 [======>.......................] - ETA: 5s
 832/3236 [======>.......................] - ETA: 4s
 896/3236 [=======>......................] - ETA: 4s
 960/3236 [=======>......................] - ETA: 4s
1024/3236 [========>.....................] - ETA: 4s
1088/3236 [=========>....................] - ETA: 3s
1152/3236 [=========>....................] - ETA: 3s
1216/3236 [==========>...................] - ETA: 3s
1280/3236 [==========>...................] - ETA: 3s
1344/3236 [===========>..................] - ETA: 3s
1408/3236 [============>.................] - ETA: 3s
1472/3236 [============>.................] - ETA: 2s
1536/3236 [=============>................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1664/3236 [==============>...............] - ETA: 2s
1728/3236 [===============>..............] - ETA: 2s
1792/3236 [===============>..............] - ETA: 2s
1856/3236 [================>.............] - ETA: 2s
1920/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 1s
2368/3236 [====================>.........] - ETA: 1s
2432/3236 [=====================>........] - ETA: 1s
2496/3236 [======================>.......] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2624/3236 [=======================>......] - ETA: 0s
2688/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2816/3236 [=========================>....] - ETA: 0s
2880/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3072/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3168/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 4s 1ms/step
The inference time is  4087.2981548309326
save time is: 5.572080612182617
total time is  39546.902894973755
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 3405.0192832946777
load data time: 4.485607147216797
process data time: 32432.76023864746

  32/3236 [..............................] - ETA: 1:09
  64/3236 [..............................] - ETA: 37s 
 128/3236 [>.............................] - ETA: 20s
 192/3236 [>.............................] - ETA: 14s
 256/3236 [=>............................] - ETA: 11s
 288/3236 [=>............................] - ETA: 10s
 320/3236 [=>............................] - ETA: 10s
 384/3236 [==>...........................] - ETA: 9s 
 448/3236 [===>..........................] - ETA: 8s
 512/3236 [===>..........................] - ETA: 7s
 576/3236 [====>.........................] - ETA: 6s
 608/3236 [====>.........................] - ETA: 6s
 672/3236 [=====>........................] - ETA: 5s
 736/3236 [=====>........................] - ETA: 5s
 832/3236 [======>.......................] - ETA: 4s
 928/3236 [=======>......................] - ETA: 4s
1024/3236 [========>.....................] - ETA: 3s
1120/3236 [=========>....................] - ETA: 3s
1216/3236 [==========>...................] - ETA: 3s
1312/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 2s
1504/3236 [============>.................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1696/3236 [==============>...............] - ETA: 2s
1792/3236 [===============>..............] - ETA: 1s
1888/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2080/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2272/3236 [====================>.........] - ETA: 1s
2336/3236 [====================>.........] - ETA: 1s
2400/3236 [=====================>........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2848/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3040/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 1ms/step
The inference time is  3309.6673488616943
save time is: 9.732246398925781
total time is  39164.09730911255
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 5675.715684890747
load data time: 3.83758544921875
process data time: 22762.61806488037

  32/3236 [..............................] - ETA: 39s
  96/3236 [..............................] - ETA: 14s
 160/3236 [>.............................] - ETA: 9s 
 224/3236 [=>............................] - ETA: 7s
 288/3236 [=>............................] - ETA: 6s
 352/3236 [==>...........................] - ETA: 5s
 416/3236 [==>...........................] - ETA: 4s
 480/3236 [===>..........................] - ETA: 4s
 576/3236 [====>.........................] - ETA: 3s
 672/3236 [=====>........................] - ETA: 3s
 768/3236 [======>.......................] - ETA: 3s
 864/3236 [=======>......................] - ETA: 2s
 960/3236 [=======>......................] - ETA: 2s
1056/3236 [========>.....................] - ETA: 2s
1120/3236 [=========>....................] - ETA: 2s
1184/3236 [=========>....................] - ETA: 2s
1280/3236 [==========>...................] - ETA: 2s
1344/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 1s
1472/3236 [============>.................] - ETA: 1s
1536/3236 [=============>................] - ETA: 1s
1632/3236 [==============>...............] - ETA: 1s
1728/3236 [===============>..............] - ETA: 1s
1824/3236 [===============>..............] - ETA: 1s
1920/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 0s
2368/3236 [====================>.........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2528/3236 [======================>.......] - ETA: 0s
2592/3236 [=======================>......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2848/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3104/3236 [===========================>..] - ETA: 0s
3200/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 1ms/step
The inference time is  3342.0915603637695
save time is: 5.576133728027344
total time is  31793.521642684937
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 4860.946893692017
load data time: 6.337642669677734
process data time: 32290.61770439148

  32/3236 [..............................] - ETA: 1:13
  96/3236 [..............................] - ETA: 26s 
 160/3236 [>.............................] - ETA: 16s
 192/3236 [>.............................] - ETA: 15s
 224/3236 [=>............................] - ETA: 14s
 288/3236 [=>............................] - ETA: 12s
 352/3236 [==>...........................] - ETA: 10s
 416/3236 [==>...........................] - ETA: 8s 
 480/3236 [===>..........................] - ETA: 7s
 544/3236 [====>.........................] - ETA: 7s
 608/3236 [====>.........................] - ETA: 6s
 672/3236 [=====>........................] - ETA: 5s
 736/3236 [=====>........................] - ETA: 5s
 800/3236 [======>.......................] - ETA: 5s
 864/3236 [=======>......................] - ETA: 4s
 928/3236 [=======>......................] - ETA: 4s
 992/3236 [========>.....................] - ETA: 4s
1056/3236 [========>.....................] - ETA: 4s
1120/3236 [=========>....................] - ETA: 3s
1184/3236 [=========>....................] - ETA: 3s
1248/3236 [==========>...................] - ETA: 3s
1312/3236 [===========>..................] - ETA: 3s
1376/3236 [===========>..................] - ETA: 3s
1408/3236 [============>.................] - ETA: 3s
1472/3236 [============>.................] - ETA: 2s
1536/3236 [=============>................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1664/3236 [==============>...............] - ETA: 2s
1760/3236 [===============>..............] - ETA: 2s
1856/3236 [================>.............] - ETA: 2s
1920/3236 [================>.............] - ETA: 1s
2016/3236 [=================>............] - ETA: 1s
2080/3236 [==================>...........] - ETA: 1s
2144/3236 [==================>...........] - ETA: 1s
2208/3236 [===================>..........] - ETA: 1s
2272/3236 [====================>.........] - ETA: 1s
2336/3236 [====================>.........] - ETA: 1s
2400/3236 [=====================>........] - ETA: 1s
2464/3236 [=====================>........] - ETA: 1s
2528/3236 [======================>.......] - ETA: 0s
2592/3236 [=======================>......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2720/3236 [========================>.....] - ETA: 0s
2784/3236 [========================>.....] - ETA: 0s
2848/3236 [=========================>....] - ETA: 0s
2912/3236 [=========================>....] - ETA: 0s
2976/3236 [==========================>...] - ETA: 0s
3040/3236 [===========================>..] - ETA: 0s
3104/3236 [===========================>..] - ETA: 0s
3168/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 4s 1ms/step
The inference time is  4303.828954696655
save time is: 13.610363006591797
total time is  41478.20782661438
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 7505.211114883423
load data time: 3.7195682525634766
process data time: 30762.845516204834

  32/3236 [..............................] - ETA: 58s
  96/3236 [..............................] - ETA: 21s
 160/3236 [>.............................] - ETA: 13s
 224/3236 [=>............................] - ETA: 10s
 288/3236 [=>............................] - ETA: 8s 
 352/3236 [==>...........................] - ETA: 7s
 416/3236 [==>...........................] - ETA: 6s
 480/3236 [===>..........................] - ETA: 6s
 544/3236 [====>.........................] - ETA: 5s
 608/3236 [====>.........................] - ETA: 5s
 672/3236 [=====>........................] - ETA: 4s
 736/3236 [=====>........................] - ETA: 4s
 800/3236 [======>.......................] - ETA: 4s
 864/3236 [=======>......................] - ETA: 4s
 928/3236 [=======>......................] - ETA: 3s
 992/3236 [========>.....................] - ETA: 3s
1056/3236 [========>.....................] - ETA: 3s
1120/3236 [=========>....................] - ETA: 3s
1184/3236 [=========>....................] - ETA: 3s
1248/3236 [==========>...................] - ETA: 3s
1312/3236 [===========>..................] - ETA: 2s
1344/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 2s
1472/3236 [============>.................] - ETA: 2s
1536/3236 [=============>................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1664/3236 [==============>...............] - ETA: 2s
1728/3236 [===============>..............] - ETA: 2s
1792/3236 [===============>..............] - ETA: 2s
1856/3236 [================>.............] - ETA: 1s
1920/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 1s
2368/3236 [====================>.........] - ETA: 1s
2432/3236 [=====================>........] - ETA: 1s
2496/3236 [======================>.......] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2624/3236 [=======================>......] - ETA: 0s
2688/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2816/3236 [=========================>....] - ETA: 0s
2880/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3072/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3200/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 4s 1ms/step
The inference time is  4142.730951309204
save time is: 7.976055145263672
total time is  42424.957275390625
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 5152.124881744385
load data time: 6.214618682861328
process data time: 27536.547660827637

  32/3236 [..............................] - ETA: 49s
  96/3236 [..............................] - ETA: 18s
 160/3236 [>.............................] - ETA: 11s
 224/3236 [=>............................] - ETA: 9s 
 288/3236 [=>............................] - ETA: 7s
 352/3236 [==>...........................] - ETA: 6s
 448/3236 [===>..........................] - ETA: 5s
 512/3236 [===>..........................] - ETA: 5s
 608/3236 [====>.........................] - ETA: 4s
 704/3236 [=====>........................] - ETA: 4s
 768/3236 [======>.......................] - ETA: 3s
 832/3236 [======>.......................] - ETA: 3s
 896/3236 [=======>......................] - ETA: 3s
 960/3236 [=======>......................] - ETA: 3s
1024/3236 [========>.....................] - ETA: 3s
1088/3236 [=========>....................] - ETA: 2s
1152/3236 [=========>....................] - ETA: 2s
1216/3236 [==========>...................] - ETA: 2s
1280/3236 [==========>...................] - ETA: 2s
1344/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 2s
1472/3236 [============>.................] - ETA: 2s
1536/3236 [=============>................] - ETA: 2s
1600/3236 [=============>................] - ETA: 2s
1664/3236 [==============>...............] - ETA: 1s
1728/3236 [===============>..............] - ETA: 1s
1792/3236 [===============>..............] - ETA: 1s
1856/3236 [================>.............] - ETA: 1s
1920/3236 [================>.............] - ETA: 1s
1984/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2272/3236 [====================>.........] - ETA: 1s
2368/3236 [====================>.........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2560/3236 [======================>.......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2752/3236 [========================>.....] - ETA: 0s
2816/3236 [=========================>....] - ETA: 0s
2912/3236 [=========================>....] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3072/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3200/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 1ms/step
The inference time is  3362.0004653930664
save time is: 5.764245986938477
total time is  36066.15400314331
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 5581.314563751221
load data time: 3.6623477935791016
process data time: 29411.105632781982

  32/3236 [..............................] - ETA: 53s
 128/3236 [>.............................] - ETA: 14s
 224/3236 [=>............................] - ETA: 8s 
 320/3236 [=>............................] - ETA: 6s
 416/3236 [==>...........................] - ETA: 5s
 512/3236 [===>..........................] - ETA: 4s
 608/3236 [====>.........................] - ETA: 4s
 704/3236 [=====>........................] - ETA: 3s
 800/3236 [======>.......................] - ETA: 3s
 896/3236 [=======>......................] - ETA: 2s
 992/3236 [========>.....................] - ETA: 2s
1088/3236 [=========>....................] - ETA: 2s
1152/3236 [=========>....................] - ETA: 2s
1216/3236 [==========>...................] - ETA: 2s
1280/3236 [==========>...................] - ETA: 2s
1344/3236 [===========>..................] - ETA: 2s
1408/3236 [============>.................] - ETA: 2s
1472/3236 [============>.................] - ETA: 1s
1536/3236 [=============>................] - ETA: 1s
1600/3236 [=============>................] - ETA: 1s
1664/3236 [==============>...............] - ETA: 1s
1760/3236 [===============>..............] - ETA: 1s
1856/3236 [================>.............] - ETA: 1s
1952/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 0s
2336/3236 [====================>.........] - ETA: 0s
2400/3236 [=====================>........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2528/3236 [======================>.......] - ETA: 0s
2624/3236 [=======================>......] - ETA: 0s
2720/3236 [========================>.....] - ETA: 0s
2784/3236 [========================>.....] - ETA: 0s
2848/3236 [=========================>....] - ETA: 0s
2944/3236 [==========================>...] - ETA: 0s
3008/3236 [==========================>...] - ETA: 0s
3104/3236 [===========================>..] - ETA: 0s
3168/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 987us/step
The inference time is  3197.770118713379
save time is: 15.646219253540039
total time is  38212.9864692688
Namespace(batch_norm=False, batch_size=32, beta_1=0.9, data='/mnt/sdc/Edge/ICU_Patient_Monitor/mimic3lite/mimic3models/in_hospital_mortality/../../data/in-hospital-mortality/', datasize='large', depth=2, dim=16, dropout=0.3, epochs=100, imputation='previous', l1=0, l2=0, load_state='mimic3models/in_hospital_mortality/keras_states/k_lstm.n16.d0.3.dep2.bs8.ts1.0.epoch23.test0.28048032904211445.state', lr=0.001, mode='test', network='mimic3models/keras_models/lstm.py', normalizer_state=None, optimizer='adam', output_dir='mimic3models/in_hospital_mortality', prefix='', rec_dropout=0.0, save_every=1, size_coef=4.0, small_part=False, target_repl_coef=0.0, timestep=1.0, verbose=2)
==> using model mimic3models/keras_models/lstm.py
==> not used params in network class: dict_keys(['batch_size', 'beta_1', 'data', 'datasize', 'epochs', 'imputation', 'l1', 'l2', 'load_state', 'lr', 'mode', 'network', 'normalizer_state', 'optimizer', 'output_dir', 'prefix', 'save_every', 'size_coef', 'small_part', 'target_repl_coef', 'timestep', 'verbose'])
==> model.final_name: k_lstm.n16.d0.3.dep2.bs32.ts1.0
==> compiling the model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
X (InputLayer)               (None, None, 76)          0         
_________________________________________________________________
masking_1 (Masking)          (None, None, 76)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 16)          5440      
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                2112      
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 17        
=================================================================
Total params: 7,569
Trainable params: 7,569
Non-trainable params: 0
_________________________________________________________________
load model time: 7550.096273422241
load data time: 3.651857376098633
process data time: 29567.888021469116

  32/3236 [..............................] - ETA: 45s
  96/3236 [..............................] - ETA: 16s
 160/3236 [>.............................] - ETA: 10s
 224/3236 [=>............................] - ETA: 8s 
 288/3236 [=>............................] - ETA: 6s
 352/3236 [==>...........................] - ETA: 5s
 416/3236 [==>...........................] - ETA: 5s
 480/3236 [===>..........................] - ETA: 4s
 544/3236 [====>.........................] - ETA: 4s
 608/3236 [====>.........................] - ETA: 4s
 672/3236 [=====>........................] - ETA: 3s
 736/3236 [=====>........................] - ETA: 3s
 800/3236 [======>.......................] - ETA: 3s
 864/3236 [=======>......................] - ETA: 3s
 928/3236 [=======>......................] - ETA: 3s
 992/3236 [========>.....................] - ETA: 2s
1056/3236 [========>.....................] - ETA: 2s
1120/3236 [=========>....................] - ETA: 2s
1216/3236 [==========>...................] - ETA: 2s
1280/3236 [==========>...................] - ETA: 2s
1376/3236 [===========>..................] - ETA: 2s
1440/3236 [============>.................] - ETA: 2s
1504/3236 [============>.................] - ETA: 1s
1568/3236 [=============>................] - ETA: 1s
1632/3236 [==============>...............] - ETA: 1s
1696/3236 [==============>...............] - ETA: 1s
1760/3236 [===============>..............] - ETA: 1s
1824/3236 [===============>..............] - ETA: 1s
1888/3236 [================>.............] - ETA: 1s
1952/3236 [=================>............] - ETA: 1s
2048/3236 [=================>............] - ETA: 1s
2112/3236 [==================>...........] - ETA: 1s
2176/3236 [===================>..........] - ETA: 1s
2240/3236 [===================>..........] - ETA: 1s
2304/3236 [====================>.........] - ETA: 0s
2400/3236 [=====================>........] - ETA: 0s
2464/3236 [=====================>........] - ETA: 0s
2528/3236 [======================>.......] - ETA: 0s
2592/3236 [=======================>......] - ETA: 0s
2656/3236 [=======================>......] - ETA: 0s
2720/3236 [========================>.....] - ETA: 0s
2784/3236 [========================>.....] - ETA: 0s
2880/3236 [=========================>....] - ETA: 0s
2976/3236 [==========================>...] - ETA: 0s
3040/3236 [===========================>..] - ETA: 0s
3072/3236 [===========================>..] - ETA: 0s
3136/3236 [============================>.] - ETA: 0s
3232/3236 [============================>.] - ETA: 0s
3236/3236 [==============================] - 3s 962us/step
The inference time is  3117.4278259277344
save time is: 5.450010299682617
total time is  40248.10171127319
